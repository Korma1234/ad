1.æ–¹æ³•ç§°è¿°
å¯¹äºé•¿æœŸæ—¶åºé¢„æµ‹ä»»åŠ¡ï¼Œå°†å…¶æ‹†è§£ä¸ºäºŒä¸ªä»»åŠ¡ï¼Œè¿‘æœŸæ—¶é—´ç‚¹çš„é¢„æŠ¥å’Œè¿œæœŸæ—¶é—´ç‚¹çš„é¢„æŠ¥ã€‚å¯¹äºè¿™ä¸¤éƒ¨åˆ†çš„é¢„æŠ¥æœ‰ä¸€ä¸ªçŒœæƒ³ï¼Œå¯¹äºçŸ­æœŸé¢„æŠ¥æ¥è¯´ï¼Œæ—¶åºç‰¹å¾ä¸­çš„è¶‹åŠ¿ç‰¹å¾ã€å­£èŠ‚ç‰¹å¾å’Œæ®‹å·®ç‰¹å¾éƒ½å¾ˆé‡è¦ï¼Œè€Œå¯¹äºè¿œæœŸé¢„æŠ¥æ¥è¯´ï¼Œè¶‹åŠ¿ç‰¹å¾å’Œå­£èŠ‚ç‰¹å¾çš„å æ¯”ä¼šæ›´é«˜ã€‚å¯¹äºè¯¥çŒœæƒ³ï¼Œé¦–å…ˆè¦æœ‰ä¸€ä¸ªä¸»å¹²æ¨¡å‹å»æ­£å¸¸çš„æå–æ—¶åºç‰¹å¾ï¼Œç„¶åè®¾è®¡ä»»åŠ¡ç‰¹å¾åŒ–çš„ç‰¹å¾è°ƒæ•´å™¨ï¼Œå¾—åˆ°é’ˆå¯¹è¿™ä¸¤ç»„ä»»åŠ¡çš„ç‰¹å¾ï¼Œç„¶åå„è‡ªå»é¢„æµ‹ï¼Œæœ€åå†åŸºäºDSè¯æ®ç†è®ºä»å¯è§£é‡Šæ€§è§’åº¦å»é›†æˆä¸¤éƒ¨åˆ†çš„é¢„æµ‹ç»“æœï¼Œä»¥è·å¾—ç»¼åˆæ›´å¥½çš„é¢„æµ‹ç»“æœã€‚
2.ç›¸å…³ä»£ç ï¼š
ï¼ˆ1ï¼‰æ¨¡å‹çš„ä»£ç integrated_three_head_model.pyï¼š
#!/usr/bin/env python3
"""
é›†æˆä¸‰å¤´Time-MoEæ¨¡å‹
åŸºäºThreeHeadTimeMoeForPredictionï¼Œæ·»åŠ å¤šç§èåˆæ–¹æ³•æ”¯æŒ
- Earlyå¤´: åªè®¡ç®—å‰32æ­¥çš„æŸå¤±
- Lateå¤´: åªè®¡ç®—å32æ­¥çš„æŸå¤±  
- Allå¤´: è®¡ç®—å…¨éƒ¨96æ­¥çš„æŸå¤±
- èåˆæ¨¡å—: æ”¯æŒMLPã€æ³¨æ„åŠ›ã€DSç­‰å¤šç§èåˆæ–¹æ³•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional, Tuple, Union, List, Dict
from transformers import PreTrainedModel
from time_moe.models.modeling_time_moe import TimeMoeForPrediction
from time_moe.models.configuration_time_moe import TimeMoeConfig
from custom_model import CustomBenchmarkDataset
from decompose_simple import OptimizedTimeSeriesDecomposition
from ds_fusion import (
    DSFusionConfig, EvidenceNet, SimpleDSStyleFusion, SoftDSFusion,
    ReliabilityEMA, LearnedReliability, MLPWeightFusion, AttentionFusion, LearnableWeightFusion,
    rank_consistency_loss, ignorance_calibration_loss, diversity_regularizer, compute_batch_error
)


class IntegratedThreeHeadTimeMoeForPrediction(PreTrainedModel):
    """
    é›†æˆä¸‰å¤´Time-MoEé¢„æµ‹æ¨¡å‹
    åœ¨åŸæœ‰ä¸‰å¤´æ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ å¤šç§èåˆæ–¹æ³•æ”¯æŒ
    """
    
    def __init__(
        self, 
        config: TimeMoeConfig, 
        horizon_length: int = 96, 
        max_length: int = 512,
        fusion_method: str = "mlp",  # "mlp", "attention", "learnable", "ds_simple", "ds_soft"
        fusion_config: Optional[DSFusionConfig] = None
    ):
        super().__init__(config)
        
        self.config = config
        self.horizon_length = horizon_length
        self.max_length = max_length
        self.input_size = config.input_size
        self.fusion_method = fusion_method
        
        # åŠ è½½é¢„è®­ç»ƒçš„Time-MoEä¸»å¹²ç½‘ç»œ
        from time_moe.models.modeling_time_moe import TimeMoeModel
        self.model = TimeMoeModel(config)
        
        # å†»ç»“ä¸»å¹²ç½‘ç»œå‚æ•°
        for param in self.model.parameters():
            param.requires_grad = False
        
        # åˆ›å»ºä¼˜åŒ–çš„æ—¶åºåˆ†è§£æ¨¡å—
        self.decomposition = OptimizedTimeSeriesDecomposition(
            feature_dim=config.hidden_size,
            seq_len=max_length,
            num_heads=8,
            dropout=0.1,
            seasonal_periods=[24, 168]  # æ—¥å‘¨æœŸå’Œå‘¨å‘¨æœŸ
        )
        
        # åˆ›å»ºä¸‰ä¸ªç‹¬ç«‹çš„é¢„æµ‹å¤´
        self.early_head = nn.Linear(config.hidden_size, horizon_length * self.input_size, bias=False)
        self.late_head = nn.Linear(config.hidden_size, horizon_length * self.input_size, bias=False)
        self.all_head = nn.Linear(config.hidden_size, horizon_length * self.input_size, bias=False)
        
        # åˆå§‹åŒ–èåˆæ¨¡å—
        self._init_fusion_modules(fusion_config)
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.training_step = 0
        
        # æŸå¤±å‡½æ•°
        self.loss_function = torch.nn.MSELoss(reduction='none')
        
        # æŸå¤±æƒé‡å‚æ•° - è°ƒæ•´ä»¥å¹³è¡¡ä¸‰ä¸ªå¤´çš„è´¡çŒ®
        self.alpha = 2.0  # Earlyå¤´æƒé‡ (æé«˜ï¼Œå› ä¸ºè¡¨ç°ç¨³å®š)
        self.beta = 1.5   # Lateå¤´æƒé‡ (å¤§å¹…æé«˜ï¼Œå› ä¸ºè¡¨ç°æœ€å·®)
        self.gamma = 0.8  # Allå¤´æƒé‡ (é™ä½ï¼Œå› ä¸ºè¿‡åº¦ä¸»å¯¼)
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
        
        print(f"ğŸ¯ é›†æˆä¸‰å¤´Time-MoEæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   é¢„æµ‹é•¿åº¦: {horizon_length}")
        print(f"   åºåˆ—é•¿åº¦: {max_length}")
        print(f"   èåˆæ–¹æ³•: {fusion_method}")
        print(f"   ç‰¹å¾åˆ†è§£: ä¼˜åŒ–ç‰ˆæœ¬ (OptimizedTimeSeriesDecomposition)")
        print(f"   è¾“å…¥ç»´åº¦: {self.input_size}")
        print(f"   éšè—ç»´åº¦: {config.hidden_size}")
        print(f"   Earlyå¤´: åªè®¡ç®—å‰{horizon_length//3}æ­¥æŸå¤± (æƒé‡Î±={self.alpha})")
        print(f"   Lateå¤´: åªè®¡ç®—å{horizon_length//3}æ­¥æŸå¤± (æƒé‡Î²={self.beta})")
        print(f"   Allå¤´: è®¡ç®—å…¨éƒ¨{horizon_length}æ­¥æŸå¤± (æƒé‡Î³={self.gamma})")
        print(f"   ä¸»å¹²ç½‘ç»œ: å·²å†»ç»“")
    
    def load_pretrained_weights(self, model_path: str):
        """
        åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°ä¸»å¹²ç½‘ç»œ
        å‚ç…§ThreeHeadTimeMoeForPredictionçš„å®ç°
        """
        print(f"ğŸ”„ åŠ è½½é¢„è®­ç»ƒæƒé‡: {model_path}")
        
        try:
            # å°è¯•åŠ è½½åŸå§‹TimeMoeForPredictionæ¨¡å‹
            from time_moe.models.modeling_time_moe import TimeMoeForPrediction
            pretrained_model = TimeMoeForPrediction.from_pretrained(model_path)
            
            # å¤åˆ¶ä¸»å¹²ç½‘ç»œæƒé‡
            self.model.load_state_dict(pretrained_model.model.state_dict(), strict=False)
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    def _init_fusion_modules(self, fusion_config: Optional[DSFusionConfig]):
        """åˆå§‹åŒ–èåˆæ¨¡å—"""
        if fusion_config is None:
            fusion_config = DSFusionConfig(
                num_experts=3,
                hidden_size=self.config.hidden_size,
                evidence_hidden=128,
                d_out=self.input_size,
                omega_mode="variance",
                use_reliability=True,
                reliability_mode="ema"
            )
        
        self.fusion_config = fusion_config
        
        if self.fusion_method == "mlp":
            # ç®€å•MLPæƒé‡èåˆ
            self.fusion_module = MLPWeightFusion(
                hidden_size=self.config.hidden_size,
                num_experts=3
            )
            
        elif self.fusion_method == "attention":
            # åŸºäºæ³¨æ„åŠ›çš„èåˆ
            self.fusion_module = AttentionFusion(
                hidden_size=self.config.hidden_size,
                num_experts=3,
                num_heads=4
            )
            
        elif self.fusion_method == "learnable":
            # å¯å­¦ä¹ æƒé‡èåˆ
            self.fusion_module = LearnableWeightFusion(
                hidden_size=self.config.hidden_size,
                num_experts=3
            )
            
        elif self.fusion_method in ["ds_simple", "ds_soft"]:
            # DSè¯æ®èåˆ
            self.evidence_net = EvidenceNet(
                hidden_size=self.config.hidden_size,
                num_experts=3,
                evidence_hidden=fusion_config.evidence_hidden,
                omega_mode=fusion_config.omega_mode
            )
            
            if self.fusion_method == "ds_simple":
                self.fusion_module = SimpleDSStyleFusion(
                    num_experts=3,
                    omega_mode=fusion_config.omega_mode,
                    omega_scale=fusion_config.omega_scale
                )
            else:  # ds_soft
                self.fusion_module = SoftDSFusion(
                    num_experts=3,
                    gamma=fusion_config.soft_ds_gamma,
                    eps=fusion_config.soft_ds_eps
                )
            
            # å¯é æ€§ä¼°è®¡
            if fusion_config.use_reliability:
                if fusion_config.reliability_mode == "ema":
                    self.reliability_ema = ReliabilityEMA(
                        num_experts=3,
                        alpha=fusion_config.ema_alpha,
                        device="cuda" if torch.cuda.is_available() else "cpu"
                    )
                elif fusion_config.reliability_mode == "learned":
                    self.learned_reliability = LearnedReliability(
                        feature_dim=self.config.hidden_size,
                        num_experts=3
                    )
        
        else:
            raise ValueError(f"Unknown fusion method: {self.fusion_method}")
    
    def forward(
        self,
        input_ids: torch.FloatTensor = None,
        inputs: torch.FloatTensor = None,  # æ·»åŠ inputså‚æ•°ç”¨äºæˆ‘ä»¬çš„æ•°æ®é›†æ ¼å¼
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,  # æ¥å—é¢å¤–å‚æ•°
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        # å¤„ç†å‚æ•°æ˜ å°„ï¼šæˆ‘ä»¬çš„æ•°æ®é›†è¿”å›'inputs'ä½†TimeMoeModelæœŸæœ›'input_ids'
        if input_ids is None and inputs is not None:
            input_ids = inputs
        
        # ä»ä¸»å¹²ç½‘ç»œè·å–éšè—çŠ¶æ€
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        
        hidden_states = outputs[0]  # [batch_size, seq_len, hidden_size]
        
        # ç‰¹å¾åˆ†è§£ï¼šåˆ†åˆ«å¾—åˆ°çŸ­æœŸå’Œé•¿æœŸç‰¹åŒ–ç‰¹å¾
        short_decomposed_features = self.decomposition(hidden_states, horizon="short")  # [batch_size, seq_len, hidden_size]
        long_decomposed_features = self.decomposition(hidden_states, horizon="long")    # [batch_size, seq_len, hidden_size]
        
        # å–åˆ†è§£åç‰¹å¾çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        original_last_hidden = hidden_states[:, -1, :]           # [batch_size, hidden_size]
        # short_last_hidden = short_decomposed_features # [batch_size, hidden_size]
        # long_last_hidden = long_decomposed_features   # [batch_size, hidden_size]
        short_last_hidden = short_decomposed_features + original_last_hidden # [batch_size, hidden_size]
        long_last_hidden = long_decomposed_features + original_last_hidden   # [batch_size, hidden_size]
        
        # ä¸‰ä¸ªé¢„æµ‹å¤´çš„å‰å‘ä¼ æ’­
        early_logits = self.early_head(short_last_hidden)  # [batch_size, horizon_length * input_size]
        late_logits = self.late_head(long_last_hidden)     # [batch_size, horizon_length * input_size]
        all_logits = self.all_head(original_last_hidden)   # [batch_size, horizon_length * input_size]
        
        # é‡å¡‘ä¸ºé¢„æµ‹å½¢çŠ¶
        batch_size = early_logits.size(0)
        early_predictions = early_logits.view(batch_size, self.horizon_length, self.input_size)
        late_predictions = late_logits.view(batch_size, self.horizon_length, self.input_size)
        all_predictions = all_logits.view(batch_size, self.horizon_length, self.input_size)
        
        # å †å ä¸“å®¶é¢„æµ‹
        expert_predictions = torch.stack([early_predictions, late_predictions, all_predictions], dim=2)  # [B, T, E, D]
        
        # åº”ç”¨èåˆæ–¹æ³•
        fused_predictions, fusion_info = self._apply_fusion(
            expert_predictions, original_last_hidden, hidden_states
        )
        
        # è®¡ç®—æŸå¤±
        total_loss = None
        if labels is not None:
            total_loss = self._compute_integrated_losses(
                early_predictions, late_predictions, all_predictions, 
                fused_predictions, labels, fusion_info
            )
        
        if not return_dict:
            # å§‹ç»ˆè¿”å›(loss, predictions)ä»¥ä¿æŒä¸€è‡´æ€§
            return (total_loss, fused_predictions)
            
        return {
            'loss': total_loss,
            'logits': fused_predictions,  # ä½¿ç”¨èåˆåçš„é¢„æµ‹ä½œä¸ºä¸»è¦è¾“å‡º
            'early_predictions': early_predictions,
            'late_predictions': late_predictions,
            'all_predictions': all_predictions,
            'fused_predictions': fused_predictions,
            'fusion_info': fusion_info,
            'hidden_states': outputs.hidden_states if output_hidden_states else None,
            'attentions': outputs.attentions if output_attentions else None,
        }
    
    def _apply_fusion(
        self, 
        expert_predictions: torch.Tensor, 
        last_hidden: torch.Tensor,
        hidden_states: torch.Tensor
    ) -> tuple:
        """
        åº”ç”¨æŒ‡å®šçš„èåˆæ–¹æ³•
        
        Args:
            expert_predictions: [B, T, E, D] ä¸“å®¶é¢„æµ‹
            last_hidden: [B, H] æœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€
            hidden_states: [B, T, H] å®Œæ•´éšè—çŠ¶æ€
            
        Returns:
            fused_predictions: [B, T, D] èåˆåçš„é¢„æµ‹
            fusion_info: dict èåˆä¿¡æ¯
        """
        if self.fusion_method in ["mlp", "learnable"]:
            # ç®€å•èåˆæ–¹æ³•
            fusion_output = self.fusion_module(expert_predictions, last_hidden)
            fused_predictions = fusion_output["fused"]
            fusion_info = {
                "method": self.fusion_method,
                "weights": fusion_output["weights"],
                "uncertainty": fusion_output["uncertainty"]
            }
            
        elif self.fusion_method == "attention":
            # æ³¨æ„åŠ›èåˆ
            fusion_output = self.fusion_module(expert_predictions, hidden_states)
            fused_predictions = fusion_output["fused"]
            fusion_info = {
                "method": self.fusion_method,
                "weights": fusion_output["weights"],
                "attention_weights": fusion_output["attention_weights"],
                "uncertainty": fusion_output["uncertainty"]
            }
            
        elif self.fusion_method in ["ds_simple", "ds_soft"]:
            # DSè¯æ®èåˆ
            evidence_logits, omega_logits = self.evidence_net(hidden_states, expert_predictions)
            
            # è·å–å¯é æ€§
            reliability = None
            if self.fusion_config.use_reliability:
                if self.fusion_config.reliability_mode == "ema" and hasattr(self, 'reliability_ema'):
                    reliability = self.reliability_ema.get().view(1, 1, -1)  # [1, 1, 3]
                    reliability = reliability.expand(expert_predictions.size(0), expert_predictions.size(1), -1)  # [B, T, 3]
                elif self.fusion_config.reliability_mode == "learned" and hasattr(self, 'learned_reliability'):
                    reliability = self.learned_reliability(hidden_states)  # [B, T, 3]
            
            # åº”ç”¨DSèåˆ
            if self.fusion_method == "ds_simple":
                fusion_output = self.fusion_module(expert_predictions, evidence_logits, omega_logits)
            else:  # ds_soft
                fusion_output = self.fusion_module(expert_predictions, evidence_logits, reliability)
            
            fused_predictions = fusion_output["fused"]
            fusion_info = {
                "method": self.fusion_method,
                "evidence_logits": evidence_logits,
                "omega_logits": omega_logits,
                "betp": fusion_output["betp"],  # Pignisticæ¦‚ç‡
                "uncertainty": fusion_output["uncert"],
                "reliability": reliability
            }
            
            if self.fusion_method == "ds_soft":
                fusion_info["conflict"] = fusion_output.get("conflict", None)
        
        else:
            raise ValueError(f"Unknown fusion method: {self.fusion_method}")
        
        return fused_predictions, fusion_info
    
    def _compute_integrated_losses(
        self,
        early_predictions: torch.Tensor,
        late_predictions: torch.Tensor,
        all_predictions: torch.Tensor,
        fused_predictions: torch.Tensor,
        labels: torch.Tensor,
        fusion_info: Dict
    ) -> torch.Tensor:
        """è®¡ç®—é›†æˆæŸå¤±"""
        
        # 1. ä¸»é¢„æµ‹æŸå¤±ï¼ˆèåˆç»“æœï¼‰
        main_loss = F.mse_loss(fused_predictions, labels)
        
        # 2. ä¸ªä½“ä¸“å®¶æŸå¤±ï¼ˆå¯é€‰ï¼Œç”¨äºç¨³å®šè®­ç»ƒï¼‰
        early_length = self.horizon_length // 3
        late_start = 2 * self.horizon_length // 3
        
        early_loss = torch.mean(self.loss_function(early_predictions[:, :early_length, :], labels[:, :early_length, :]))
        late_loss = torch.mean(self.loss_function(late_predictions[:, late_start:, :], labels[:, late_start:, :]))
        all_loss = torch.mean(self.loss_function(all_predictions, labels))
        
        individual_loss = self.alpha * early_loss + self.beta * late_loss + self.gamma * all_loss
        
        # 3. èåˆç‰¹å®šæŸå¤±
        fusion_loss = torch.tensor(0.0, device=main_loss.device)
        
        if self.fusion_method in ["ds_simple", "ds_soft"]:
            # DSèåˆçš„é¢å¤–æŸå¤±
            expert_predictions = torch.stack([early_predictions, late_predictions, all_predictions], dim=2)
            
            # æ’åºä¸€è‡´æ€§æŸå¤±
            if "betp" in fusion_info:
                rank_loss = rank_consistency_loss(
                    fusion_info["betp"], expert_predictions, labels
                ) * self.fusion_config.lambda_rank
                fusion_loss += rank_loss
            
            # æ— çŸ¥æ ¡å‡†æŸå¤±
            if "uncertainty" in fusion_info and "ignorance" in fusion_info["uncertainty"]:
                calib_loss = ignorance_calibration_loss(
                    fusion_info["uncertainty"]["ignorance"], expert_predictions, labels
                ) * self.fusion_config.lambda_calib
                fusion_loss += calib_loss
            
            # å¤šæ ·æ€§æ­£åˆ™åŒ–
            if "betp" in fusion_info:
                div_loss = diversity_regularizer(fusion_info["betp"]) * self.fusion_config.lambda_div
                fusion_loss += div_loss
        
        # æ€»æŸå¤±
        total_loss = main_loss + 0.1 * individual_loss + fusion_loss
        
        return total_loss
    
    def update_reliability(self, expert_predictions: torch.Tensor, labels: torch.Tensor):
        """æ›´æ–°EMAå¯é æ€§ä¼°è®¡ï¼ˆä»…ç”¨äºDSèåˆï¼‰"""
        if (self.fusion_method in ["ds_simple", "ds_soft"] and 
            hasattr(self, 'reliability_ema')):
            
            # è®¡ç®—æ‰¹æ¬¡è¯¯å·®
            batch_err_mean = compute_batch_error(expert_predictions, labels)
            self.reliability_ema.update(batch_err_mean.detach())
    
    def get_fusion_weights(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """è·å–å½“å‰èåˆæƒé‡ï¼ˆç”¨äºåˆ†æï¼‰"""
        with torch.no_grad():
            last_hidden = hidden_states[:, -1, :]
            
            if self.fusion_method == "mlp":
                return self.fusion_module.fusion_net(last_hidden)
            elif self.fusion_method == "learnable":
                static_weights = F.softmax(self.fusion_module.learnable_weights, dim=-1)
                dynamic_weights = self.fusion_module.weight_net(last_hidden)
                return 0.7 * static_weights.unsqueeze(0) + 0.3 * dynamic_weights
            else:
                # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œè¿”å›å¹³å‡æƒé‡
                return torch.ones(hidden_states.size(0), 3, device=hidden_states.device) / 3


class IntegratedThreeHeadTimeMoeRunner:
    """
    é›†æˆä¸‰å¤´Time-MoEæ¨¡å‹è®­ç»ƒå™¨
    åŸºäºThreeHeadTimeMoeRunnerï¼Œæ·»åŠ èåˆæ–¹æ³•æ”¯æŒ
    """
    
    def __init__(
        self,
        data_path: str,
        model_path: str,
        output_path: str,
        fusion_method: str = "mlp",
        horizon_length: int = 96,
        max_length: int = 512,
        num_epochs: int = 3,
        learning_rate: float = 1e-4,
        min_learning_rate: float = 1e-5,
        micro_batch_size: int = 4,
        global_batch_size: int = 32,
        logging_steps: int = 10,
        evaluation_strategy: str = "no",
        eval_steps: int = 100,
        dataloader_num_workers: int = 16,
        precision: str = "fp32",
        seed: int = 9989,
        freeze_backbone: bool = True,
        mix_test_samples: bool = False,
        test_mix_ratio: float = 0.1,
        test_mix_seed: int = None,
        fusion_config: Optional[DSFusionConfig] = None,
        **kwargs
    ):
        # å¯¼å…¥åŸå§‹è®­ç»ƒå™¨
        from three_head_model import ThreeHeadTimeMoeRunner
        
        # åˆå§‹åŒ–åŸå§‹è®­ç»ƒå™¨
        self.base_runner = ThreeHeadTimeMoeRunner(
            data_path=data_path,
            model_path=model_path,
            output_path=output_path,
            horizon_length=horizon_length,
            max_length=max_length,
            num_epochs=num_epochs,
            learning_rate=learning_rate,
            min_learning_rate=min_learning_rate,
            micro_batch_size=micro_batch_size,
            global_batch_size=global_batch_size,
            logging_steps=logging_steps,
            evaluation_strategy=evaluation_strategy,
            eval_steps=eval_steps,
            dataloader_num_workers=dataloader_num_workers,
            precision=precision,
            seed=seed,
            freeze_backbone=freeze_backbone,
            mix_test_samples=mix_test_samples,
            test_mix_ratio=test_mix_ratio,
            test_mix_seed=test_mix_seed,
            **kwargs
        )
        
        # æ·»åŠ èåˆç›¸å…³å‚æ•°
        self.fusion_method = fusion_method
        self.fusion_config = fusion_config or DSFusionConfig()
        
        print(f"ğŸ”§ é›†æˆæ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   èåˆæ–¹æ³•: {fusion_method}")
        print(f"   èåˆé…ç½®: {self.fusion_config}")
    
    def create_model(self):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        # åŠ è½½é…ç½®
        config = TimeMoeConfig.from_pretrained(self.base_runner.model_path)
        
        # åˆ›å»ºé›†æˆæ¨¡å‹
        model = IntegratedThreeHeadTimeMoeForPrediction(
            config=config,
            horizon_length=self.base_runner.horizon_length,
            max_length=self.base_runner.max_length,
            fusion_method=self.fusion_method,
            fusion_config=self.fusion_config
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ - è¿™æ˜¯å…³é”®ï¼
        model.load_pretrained_weights(self.base_runner.model_path)
        
        return model
    
    def train_model(self):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹ (èåˆæ–¹æ³•: {self.fusion_method})")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = self.base_runner.get_train_dataset()
        eval_dataset = self.base_runner.get_eval_dataset()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        from transformers import TrainingArguments, Trainer
        
        training_args = TrainingArguments(
            output_dir=self.base_runner.output_path,
            num_train_epochs=self.base_runner.num_epochs,
            per_device_train_batch_size=self.base_runner.micro_batch_size,
            per_device_eval_batch_size=self.base_runner.micro_batch_size,
            gradient_accumulation_steps=self.base_runner.global_batch_size // (self.base_runner.micro_batch_size * 8),  # 8 GPUs
            learning_rate=self.base_runner.learning_rate,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            weight_decay=0.01,
            adam_beta1=0.9,
            adam_beta2=0.95,
            adam_epsilon=1e-08,
            max_grad_norm=1.0,
            logging_steps=self.base_runner.logging_steps,
            evaluation_strategy=self.base_runner.evaluation_strategy,
            eval_steps=self.base_runner.eval_steps,
            save_strategy="steps",
            save_steps=500,
            save_total_limit=3,
            load_best_model_at_end=False,  # ä¸åŸå§‹ä»£ç ä¿æŒä¸€è‡´
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_num_workers=self.base_runner.dataloader_num_workers,
            fp16=(self.base_runner.precision == "fp16"),
            bf16=(self.base_runner.precision == "bf16"),
            seed=self.base_runner.seed,
            remove_unused_columns=False,
            report_to=None,
        )
        
        # åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ç°
        class MinLRTrainer(Trainer):
            def __init__(self, min_learning_rate, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.min_learning_rate = min_learning_rate
            
            def create_scheduler(self, num_training_steps, optimizer=None):
                optimizer = self.optimizer if optimizer is None else optimizer
                min_lr_ratio = self.min_learning_rate / self.args.learning_rate
                
                if self.lr_scheduler is None:
                    if self.args.lr_scheduler_type == 'cosine':
                        self.lr_scheduler = self._get_cosine_schedule_with_warmup_min_lr(
                            optimizer=optimizer,
                            num_warmup_steps=self.args.get_warmup_steps(num_training_steps),
                            num_training_steps=num_training_steps,
                            min_lr_ratio=min_lr_ratio,
                        )
                    else:
                        from transformers import get_scheduler
                        self.lr_scheduler = get_scheduler(
                            self.args.lr_scheduler_type,
                            optimizer=optimizer,
                            num_warmup_steps=self.args.get_warmup_steps(num_training_steps),
                            num_training_steps=num_training_steps,
                        )
                    self._created_lr_scheduler = True
                return self.lr_scheduler
            
            def _get_cosine_schedule_with_warmup_min_lr(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, min_lr_ratio=0, last_epoch=-1):
                """
                åˆ›å»ºå¸¦æœ‰æœ€å°å­¦ä¹ ç‡çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨
                """
                import math
                from torch.optim.lr_scheduler import LambdaLR
                
                def lr_lambda(current_step):
                    if current_step < num_warmup_steps:
                        return float(current_step) / float(max(1, num_warmup_steps))
                    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
                    return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
                
                return LambdaLR(optimizer, lr_lambda, last_epoch)
            
            def log(self, logs):
                # è®°å½•å½“å‰å­¦ä¹ ç‡
                if hasattr(self.lr_scheduler, 'get_last_lr'):
                    current_lr = self.lr_scheduler.get_last_lr()[0]
                else:
                    current_lr = self.args.learning_rate
                
                logs['learning_rate'] = current_lr
                
                # è°ƒç”¨çˆ¶ç±»çš„logæ–¹æ³•
                super().log(logs)
                
                # é¢å¤–è®°å½•å­¦ä¹ ç‡åˆ°æ–‡ä»¶
                if self.state.global_step % 10 == 0:
                    with open('integrated_3head_training_loss_log.txt', 'a', encoding='utf-8') as f:
                        f.write(f"ğŸ“ˆ Step {self.state.global_step}: å­¦ä¹ ç‡ = {current_lr:.8f}\n")
        
        trainer = MinLRTrainer(
            min_learning_rate=self.base_runner.min_learning_rate,
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {self.base_runner.output_path}")
        
        return trainer, model
    
    def get_train_dataset(self):
        """è·å–è®­ç»ƒæ•°æ®é›†"""
        return self.base_runner.get_train_dataset()
    
    def get_eval_dataset(self):
        """è·å–è¯„ä¼°æ•°æ®é›†"""
        return self.base_runner.get_eval_dataset()


def evaluate_integrated_model(model, eval_dataset, device, batch_size=32, max_samples=0):
    """
    è¯„ä¼°é›†æˆä¸‰å¤´æ¨¡å‹ï¼š
    - åˆ†åˆ«è¯„ä¼° Early/Late/All ä¸‰ä¸ªå¤´çš„åˆ†æ®µä¸æ•´ä½“æ€§èƒ½
    - è¯„ä¼°èåˆé¢„æµ‹çš„åˆ†æ®µä¸æ•´ä½“æ€§èƒ½
    è¾“å‡ºåŒ…å«å››è·¯ï¼ˆearly/late/all/fusedï¼‰çš„ overall/early/late çš„ MSE/MAE æŒ‡æ ‡
    """
    import json
    model.eval()

    early_length = model.horizon_length // 3
    late_start = 2 * model.horizon_length // 3

    # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°
    total_samples = len(eval_dataset)
    if max_samples is not None and max_samples > 0:
        total_samples = min(total_samples, max_samples)

    # æ”¶é›†é¢„æµ‹
    all_early, all_late, all_all, all_fused, all_labels = [], [], [], [], []

    with torch.no_grad():
        for batch_start in range(0, total_samples, batch_size):
            batch_end = min(batch_start + batch_size, total_samples)
            batch_inputs = []
            batch_labels = []

            for i in range(batch_start, batch_end):
                sample = eval_dataset[i]
                batch_inputs.append(torch.from_numpy(sample['inputs']))
                batch_labels.append(torch.from_numpy(sample['labels']))

            inputs = torch.stack(batch_inputs).to(device)      # [B, Tin, Din]
            labels = torch.stack(batch_labels).to(device)      # [B, Tout, Dout]

            outputs = model(inputs=inputs)  # forward è¿”å› dict

            all_early.append(outputs['early_predictions'].detach().cpu())
            all_late.append(outputs['late_predictions'].detach().cpu())
            all_all.append(outputs['all_predictions'].detach().cpu())
            # å…¼å®¹é”®åï¼ˆæœ‰çš„ä»£ç ç”¨ logitsï¼Œæœ‰çš„ç”¨ fused_predictionsï¼‰
            fused = outputs.get('fused_predictions', None)
            if fused is None:
                fused = outputs.get('logits')
            all_fused.append(fused.detach().cpu())
            all_labels.append(labels.detach().cpu())

    early_preds = torch.cat(all_early, dim=0)
    late_preds = torch.cat(all_late, dim=0)
    all_preds = torch.cat(all_all, dim=0)
    fused_preds = torch.cat(all_fused, dim=0)
    labels = torch.cat(all_labels, dim=0)

    def compute_metrics(preds, labs):
        metrics = {}
        metrics['overall_mse'] = torch.mean((preds - labs) ** 2).item()
        metrics['overall_mae'] = torch.mean(torch.abs(preds - labs)).item()
        metrics['early_mse'] = torch.mean((preds[:, :early_length, :] - labs[:, :early_length, :]) ** 2).item()
        metrics['early_mae'] = torch.mean(torch.abs(preds[:, :early_length, :] - labs[:, :early_length, :])).item()
        metrics['late_mse'] = torch.mean((preds[:, late_start:, :] - labs[:, late_start:, :]) ** 2).item()
        metrics['late_mae'] = torch.mean(torch.abs(preds[:, late_start:, :] - labs[:, late_start:, :])).item()
        return metrics

    results = {
        'early_head': compute_metrics(early_preds, labels),
        'late_head': compute_metrics(late_preds, labels),
        'all_head': compute_metrics(all_preds, labels),
        'fused': compute_metrics(fused_preds, labels),
        'fusion_method': getattr(model, 'fusion_method', None),
        'fusion_config': getattr(model, 'fusion_config', None).__dict__ if hasattr(model, 'fusion_config') else None,
    }

    # ä¿å­˜
    with open('evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2)

    return results
ï¼ˆ2ï¼‰æ¨¡å—ä»£ç ds_fusion.pyå’Œdecompose_simple.pyï¼š
1)
#!/usr/bin/env python3
"""
DSè¯æ®èåˆæ¨¡å—
åŸºäºDempster-Shaferè¯æ®ç†è®ºçš„å¤šä¸“å®¶èåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, List, Union
from dataclasses import dataclass


@dataclass
class DSFusionConfig:
    """DSèåˆé…ç½®"""
    num_experts: int = 3
    hidden_size: int = 256
    evidence_hidden: int = 128
    d_out: int = 1
    omega_mode: str = "variance"     # "variance" | "learned" | "constant"
    omega_scale: float = 1.0
    use_reliability: bool = True
    reliability_mode: str = "ema"    # "ema" | "learned" | "none"
    ema_alpha: float = 0.9
    soft_ds_gamma: float = 0.8
    soft_ds_eps: float = 1e-6
    
    # Loss æƒé‡
    lambda_rank: float = 0.2
    lambda_calib: float = 0.1
    lambda_div: float = 0.0


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model: int, max_len: int = 10000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).float().unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()
                             * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        T = x.size(1)
        return x + self.pe[:, :T, :]


class EvidenceNet(nn.Module):
    """è¯æ®ç½‘ç»œï¼šç”±éšè—çŠ¶æ€è¾“å‡ºä¸“å®¶è¯æ®logits"""
    def __init__(self, hidden_size: int, num_experts: int, evidence_hidden: int = 128, omega_mode: str = "variance"):
        super().__init__()
        self.num_experts = num_experts
        self.omega_mode = omega_mode
        
        # è¯æ®ç”Ÿæˆç½‘ç»œ
        self.proj = nn.Sequential(
            nn.Linear(hidden_size, evidence_hidden),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(evidence_hidden, num_experts)
        )
        
        # Omegaè¯æ®ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
        if omega_mode == "learned":
            self.omega_head = nn.Sequential(
                nn.Linear(hidden_size, evidence_hidden),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(evidence_hidden, 1)
            )
        else:
            self.omega_head = None
        
        self.pos_enc = PositionalEncoding(hidden_size)

    def forward(self, Z: torch.Tensor, expert_preds: Optional[torch.Tensor] = None):
        """
        Z: [B,seq_len,H] éšè—çŠ¶æ€
        expert_preds: [B,T,E,D] ä¸“å®¶é¢„æµ‹ï¼ˆå¯é€‰ï¼Œç”¨äºvarianceæ¨¡å¼ï¼‰
        """
        if expert_preds is not None:
            B, T, E, D = expert_preds.shape
            # ä½¿ç”¨Zçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºå…¨å±€ç‰¹å¾
            last_hidden = Z[:, -1, :]  # [B, H]
            # å¹¿æ’­åˆ°æ‰€æœ‰é¢„æµ‹æ—¶é—´æ­¥
            Z_expanded = last_hidden.unsqueeze(1).expand(-1, T, -1)  # [B, T, H]
        else:
            Z_expanded = Z
        
        Zc = self.pos_enc(Z_expanded)
        evidence_logits = self.proj(Zc)  # [B,T,E]
        
        omega_logits = None
        if self.omega_mode == "learned":
            omega_logits = self.omega_head(Zc)  # [B,T,1]
        
        return evidence_logits, omega_logits


class ReliabilityEMA:
    """EMAå¯é æ€§ä¼°è®¡"""
    def __init__(self, num_experts: int, alpha: float = 0.9, device: str = "cpu"):
        self.alpha = alpha
        self.num_experts = num_experts
        self.device = device
        self.register()

    def register(self):
        self.running = torch.ones(self.num_experts, device=self.device)

    def update(self, batch_err_mean: torch.Tensor):
        # batch_err_mean: [E]
        self.running = self.alpha * self.running + (1 - self.alpha) * batch_err_mean

    def get(self) -> torch.Tensor:
        inv = 1.0 / (self.running + 1e-6)
        r = inv / (inv.sum() + 1e-9)  # [E]
        return r


class LearnedReliability(nn.Module):
    """å¯å­¦ä¹ å¯é æ€§ä¼°è®¡"""
    def __init__(self, feature_dim: int, num_experts: int, hidden: int = 64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(feature_dim, hidden),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden, num_experts),
            nn.Sigmoid()
        )

    def forward(self, feat: torch.Tensor) -> torch.Tensor:
        """
        feat: [B,T,F]
        return: [B,T,E] in (0,1)
        """
        return self.net(feat)


class SimpleDSStyleFusion(nn.Module):
    """ç®€å•DSé£æ ¼èåˆï¼ˆä¸æ‰§è¡Œè¿­ä»£ç»„åˆï¼‰"""
    def __init__(self, num_experts: int, omega_mode: str = "variance", omega_scale: float = 1.0):
        super().__init__()
        self.num_experts = num_experts
        self.omega_mode = omega_mode
        self.omega_scale = omega_scale

    def forward(self, expert_preds: torch.Tensor, evidence_logits: torch.Tensor, omega_logits: Optional[torch.Tensor] = None):
        """
        expert_preds:    [B,T,E,D]
        evidence_logits: [B,T,E]
        omega_logits:    [B,T,1]  (learned æ¨¡å¼)
        """
        B, T, E, D = expert_preds.shape
        evidence = F.softplus(evidence_logits)  # ä¿è¯éè´Ÿ

        # ç”Ÿæˆ Omega è¯æ®
        if self.omega_mode == "learned":
            assert omega_logits is not None, "omega_mode=learned éœ€ä¼ å…¥ omega_logits"
            e_omega = F.softplus(omega_logits)
        elif self.omega_mode == "variance":
            # ä½¿ç”¨ä¸“å®¶é¢„æµ‹çš„æ–¹å·®ä½œä¸ºæ— çŸ¥è¯æ®
            mean_each = expert_preds.mean(dim=-1)          # [B,T,E]
            var = torch.var(mean_each, dim=2, keepdim=True)  # [B,T,1]
            e_omega = self.omega_scale * (var + 1e-6)
        elif self.omega_mode == "constant":
            e_omega = torch.ones(B, T, 1, device=expert_preds.device)
        else:
            raise ValueError("Unknown omega_mode")

        # å½’ä¸€åŒ–
        denom = evidence.sum(dim=-1, keepdim=True) + e_omega  # [B,T,1]
        m_experts = evidence / (denom + 1e-9)                 # [B,T,E]
        m_omega = e_omega / (denom + 1e-9)                   # [B,T,1]

        # Pignistic æ¦‚ç‡
        betp = m_experts + m_omega / self.num_experts        # [B,T,E]
        fused = torch.einsum("bte,bted->btd", betp, expert_preds)

        out = {
            "fused": fused,          # [B,T,D]
            "betp": betp,            # [B,T,E]
            "m_experts": m_experts,  # [B,T,E]
            "m_omega": m_omega,      # [B,T,1]
            "uncert": {
                "ignorance": m_omega,
                "entropy": -(betp * (betp + 1e-9).log()).sum(-1, keepdim=True),
                "divergence": torch.var(mean_each, dim=2, keepdim=True)
            }
        }
        return out


class SoftDSFusion(nn.Module):
    """Soft-DSè¿­ä»£ç»„åˆèåˆï¼ˆå†²çªæ„ŸçŸ¥ï¼‰"""
    def __init__(self, num_experts: int, gamma: float = 0.8, eps: float = 1e-6):
        super().__init__()
        self.num_experts = num_experts
        self.gamma = gamma
        self.eps = eps

    def forward(self, expert_preds: torch.Tensor, evidence_logits: torch.Tensor, reliability: Optional[torch.Tensor] = None):
        """
        expert_preds:    [B,T,E,D]
        evidence_logits: [B,T,E]
        reliability:     [B,T,E] (å‰ç½®æŠ˜æ‰£ï¼Œå¯ None)
        """
        B, T, E, D = expert_preds.shape
        evidence = F.softplus(evidence_logits)
        if reliability is not None:
            evidence = evidence * torch.clamp(reliability, 0.0, 1.0)

        p = evidence / (evidence.sum(dim=-1, keepdim=True) + 1e-9)  # [B,T,E]

        # æ„é€ å•æº
        sources: List[Dict[int, torch.Tensor]] = []
        for i in range(E):
            src = {}
            for j in range(E):
                if j == i:
                    src[j] = p[..., i]  # [B,T]
                else:
                    src[j] = torch.zeros_like(p[..., i])  # ä½¿ç”¨p[..., i]è€Œä¸æ˜¯p[..., 0]
            src['Omega'] = 1 - p[..., i]
            sources.append(src)

        def combine_pair(m1, m2):
            # m1,m2: dict {0:[B,T],1:[B,T],..., 'Omega':[B,T]}
            K = torch.zeros(B, T, device=p.device)
            for i in range(E):
                for j in range(E):
                    if i != j:
                        K += m1[i] * m2[j]
            Dn = (1 - K).clamp(min=0)
            D = torch.pow(Dn, self.gamma) + self.eps
            m_new = {}
            for k in range(E):
                num = m1[k]*m2[k] + m1[k]*m2['Omega'] + m1['Omega']*m2[k]
                m_new[k] = num / D
            m_new['Omega'] = (m1['Omega'] * m2['Omega']) / D
            return m_new, K

        # è¿­ä»£ç»„åˆ
        current = sources[0]
        K_list = []
        for i in range(1, E):
            current, K = combine_pair(current, sources[i])
            K_list.append(K.unsqueeze(-1))
        K_all = torch.cat(K_list, dim=-1) if K_list else torch.zeros(B, T, 1, device=p.device)

        # Pignistic
        betp_list = [current[i] + current['Omega'] / E for i in range(E)]
        betp = torch.stack(betp_list, dim=-1)  # [B,T,E]
        fused = torch.einsum("bte,bted->btd", betp, expert_preds)

        mean_each = expert_preds.mean(dim=-1)  # [B,T,E]
        out = {
            "fused": fused,
            "betp": betp,
            "m_final": current,
            "conflict": K_all,  # æ¯æ¬¡ç»„åˆçš„ K
            "uncert": {
                "ignorance": current['Omega'].unsqueeze(-1),
                "entropy": -(betp * (betp + 1e-9).log()).sum(-1, keepdim=True),
                "divergence": torch.var(mean_each, dim=2, keepdim=True)
            }
        }
        return out


class MLPWeightFusion(nn.Module):
    """ç®€å•MLPæƒé‡èåˆ"""
    def __init__(self, hidden_size: int, num_experts: int = 3):
        super().__init__()
        self.num_experts = num_experts
        self.fusion_net = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, num_experts),
            nn.Softmax(dim=-1)
        )

    def forward(self, expert_preds: torch.Tensor, last_hidden: torch.Tensor):
        """
        expert_preds: [B,T,E,D]
        last_hidden: [B,H]
        """
        weights = self.fusion_net(last_hidden)  # [B, E]
        weights = weights.unsqueeze(1).unsqueeze(-1)  # [B, 1, E, 1]
        fused = torch.sum(expert_preds * weights, dim=2)  # [B, T, D]
        
        return {
            "fused": fused,
            "weights": weights.squeeze(1).squeeze(-1),  # [B, E]
            "uncertainty": None
        }


class AttentionFusion(nn.Module):
    """åŸºäºæ³¨æ„åŠ›çš„èåˆ - åœ¨ä¸“å®¶ç»´åº¦ä¸Šåšè‡ªæ³¨æ„åŠ›ï¼Œä¿æŒDç»´åº¦ä¸å˜"""
    def __init__(self, hidden_size: int, num_experts: int = 3, num_heads: int = 4):
        super().__init__()
        self.num_experts = num_experts
        self.hidden_size = 96 # pred len
        
        # ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸“å®¶ç»´åº¦ä¸Šè¿›è¡Œèåˆ
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†Tç»´åº¦ä½œä¸ºembed_dimï¼Œå› ä¸ºæˆ‘ä»¬è¦åœ¨Eç»´åº¦ä¸Šåšæ³¨æ„åŠ›
        self.attention_fusion = nn.MultiheadAttention(
            embed_dim=self.hidden_size,  # è¿™é‡Œåº”è¯¥æ˜¯Tç»´åº¦ï¼Œä½†MultiheadAttentionéœ€è¦å›ºå®šç»´åº¦
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # èåˆæƒé‡è®¡ç®— - è¾“å…¥ç»´åº¦åº”è¯¥æ˜¯Dï¼Œä¸æ˜¯hidden_size
        self.weight_net = nn.Sequential(
            nn.Linear(1, 16),  # è¾“å…¥æ˜¯D=1ï¼Œè¾“å‡º16ç»´
            nn.ReLU(),
            nn.Linear(16, num_experts)  # è¾“å‡ºnum_experts=3ç»´
        )

    def forward(self, expert_preds: torch.Tensor, hidden_states: torch.Tensor):
        """
        expert_preds: [B,T,E,D]
        hidden_states: [B,T,H]
        """
        B, T, E, D = expert_preds.shape
        
        # é‡å¡‘ä¸º [B*D, E, T] è¿›è¡Œä¸“å®¶ç»´åº¦çš„æ³¨æ„åŠ›
        # å°†Dç»´åº¦ä¸Bç»´åº¦åˆå¹¶ï¼ŒEä½œä¸ºåºåˆ—é•¿åº¦ï¼ŒTä½œä¸ºç‰¹å¾ç»´åº¦
        expert_preds_reshaped = expert_preds.permute(0, 3, 2, 1)  # [B, D, E, T]
        expert_preds_flat = expert_preds_reshaped.contiguous().view(B*D, E, T)  # [B*D, E, T]
        
        # è‡ªæ³¨æ„åŠ›ï¼šåœ¨ä¸“å®¶ç»´åº¦ä¸Šå­¦ä¹ ä¸“å®¶é—´çš„å…³ç³»
        attn_output, attn_weights = self.attention_fusion(
            query=expert_preds_flat,  # [B*D, E, T]
            key=expert_preds_flat,    # [B*D, E, T] 
            value=expert_preds_flat   # [B*D, E, T]
        )
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        attn_output = attn_output.view(B*D, E, T)  # [B*D, E, T]
        attn_output = attn_output.permute(0, 2, 1)  # [B*D, T, E]
        attn_output = attn_output.view(B, D, T, E)  # [B, D, T, E]
        attn_output = attn_output.permute(0, 2, 3, 1)  # [B, T, E, D]
        
        # è®¡ç®—èåˆæƒé‡ - åŸºäºæ³¨æ„åŠ›è¾“å‡ºçš„å¹³å‡
        attn_mean = attn_output.mean(dim=1)  # [B, E, D]
        attn_mean_flat = attn_mean.view(B*E, D)  # [B*E, D]
        fusion_weights_flat = self.weight_net(attn_mean_flat)  # [B*E, num_experts]
        fusion_weights = fusion_weights_flat.view(B, E, self.num_experts)  # [B, E, num_experts]
        fusion_weights = F.softmax(fusion_weights, dim=-1)
        
        # åº”ç”¨æƒé‡ - ä½¿ç”¨å¯¹è§’æƒé‡ï¼ˆæ¯ä¸ªä¸“å®¶å¯¹åº”è‡ªå·±çš„æƒé‡ï¼‰
        expert_weights = torch.diagonal(fusion_weights, dim1=1, dim2=2)  # [B, E]
        weights = expert_weights.unsqueeze(1).unsqueeze(-1)  # [B, 1, E, 1]
        fused = torch.sum(attn_output * weights, dim=2)  # [B, T, D]
        
        # è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆåŸºäºæ³¨æ„åŠ›æƒé‡çš„ç†µï¼‰
        uncertainty = -torch.sum(attn_weights * torch.log(attn_weights + 1e-8), dim=-1)
        uncertainty = uncertainty.mean(dim=1)  # [B*D]
        uncertainty = uncertainty.view(B, D).mean(dim=1)  # [B]
        
        return {
            "fused": fused,
            "weights": expert_weights,  # [B, E]
            "attention_weights": attn_weights,
            "uncertainty": uncertainty
        }


class LearnableWeightFusion(nn.Module):
    """å¯å­¦ä¹ æƒé‡èåˆ"""
    def __init__(self, hidden_size: int, num_experts: int = 3):
        super().__init__()
        self.num_experts = num_experts
        self.learnable_weights = nn.Parameter(torch.ones(num_experts) / num_experts)
        self.weight_net = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, num_experts),
            nn.Softmax(dim=-1)
        )

    def forward(self, expert_preds: torch.Tensor, last_hidden: torch.Tensor):
        """
        expert_preds: [B,T,E,D]
        last_hidden: [B,H]
        """
        # ç»“åˆé™æ€æƒé‡å’ŒåŠ¨æ€æƒé‡
        static_weights = F.softmax(self.learnable_weights, dim=-1)  # [E]
        dynamic_weights = self.weight_net(last_hidden)  # [B, E]
        
        # æ··åˆæƒé‡
        mixed_weights = 0.7 * static_weights.unsqueeze(0) + 0.3 * dynamic_weights  # [B, E]
        mixed_weights = F.softmax(mixed_weights, dim=-1)
        
        weights = mixed_weights.unsqueeze(1).unsqueeze(-1)  # [B, 1, E, 1]
        fused = torch.sum(expert_preds * weights, dim=2)  # [B, T, D]
        
        return {
            "fused": fused,
            "weights": mixed_weights,  # æ·»åŠ weightsé”®ï¼Œä½¿ç”¨mixed_weights
            "static_weights": static_weights,
            "dynamic_weights": dynamic_weights,
            "mixed_weights": mixed_weights,
            "uncertainty": None
        }


# æŸå¤±å‡½æ•°
def rank_consistency_loss(betp: torch.Tensor, expert_preds: torch.Tensor, target: torch.Tensor, margin: float = 0.0) -> torch.Tensor:
    """æ’åºä¸€è‡´æ€§æŸå¤±"""
    with torch.no_grad():
        err = ((expert_preds - target.unsqueeze(2)) ** 2).mean(dim=-1)  # [B,T,E]
    B, T, E = betp.shape
    terms = []
    for i in range(E):
        for j in range(E):
            if i == j:
                continue
            mask = (err[..., i] < err[..., j]).float()
            violation = F.relu(betp[..., j] - betp[..., i] + margin)
            terms.append(violation * mask)
    if not terms:
        return torch.tensor(0.0, device=betp.device)
    loss = torch.stack(terms, dim=0).mean()
    return loss


def ignorance_calibration_loss(ignorance: torch.Tensor, expert_preds: torch.Tensor, target: torch.Tensor, detach_pred: bool = True) -> torch.Tensor:
    """æ— çŸ¥æ ¡å‡†æŸå¤±"""
    err = ((expert_preds.mean(dim=2) - target) ** 2).mean(dim=-1, keepdim=True)  # [B,T,1]
    if detach_pred:
        err = err.detach()
    ig = ignorance
    ig_c = ig - ig.mean()
    err_c = err - err.mean()
    num = (ig_c * err_c).mean()
    denom = (ig_c.pow(2).mean().sqrt() * err_c.pow(2).mean().sqrt() + 1e-9)
    corr = num / denom
    return (1 - corr).clamp(min=0.0)


def diversity_regularizer(betp: torch.Tensor) -> torch.Tensor:
    """å¤šæ ·æ€§æ­£åˆ™åŒ–"""
    entropy = -(betp * (betp + 1e-9).log()).sum(dim=-1)  # [B,T]
    return -entropy.mean()


def compute_batch_error(expert_preds: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """è®¡ç®—æ‰¹æ¬¡è¯¯å·®"""
    err = (expert_preds - target.unsqueeze(2)) ** 2  # [B,T,E,D]
    mse = err.mean(dim=(0, 1, 3))
    return mse

2)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class OptimizedTimeSeriesDecomposition(nn.Module):
    def __init__(self, feature_dim, seq_len, num_heads=4, dropout=0.1, seasonal_periods=[24, 168]):
        """
        é’ˆå¯¹æœ€åæ—¶é—´æ­¥é¢„æµ‹ä¼˜åŒ–çš„æ—¶åºåˆ†è§£æ¨¡å—
        
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦
            seq_len: åºåˆ—é•¿åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°é‡
            dropout: Dropoutæ¯”ç‡
            seasonal_periods: å­£èŠ‚æ€§å‘¨æœŸåˆ—è¡¨
        """
        super(OptimizedTimeSeriesDecomposition, self).__init__()
        self.feature_dim = feature_dim
        self.seq_len = seq_len
        self.seasonal_periods = seasonal_periods
        
        # 1. è½»é‡çº§è¶‹åŠ¿æå–å™¨ - åªå…³æ³¨ä¸æœ€åæ—¶é—´æ­¥ç›¸å…³çš„è®¡ç®—
        self.trend_extractor = LastStepTrendExtractor(
            feature_dim=feature_dim,
            seq_len=seq_len,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # 2. èšç„¦å‹å­£èŠ‚æ€§æå–å™¨ - åªæå–å…³é”®å‘¨æœŸä½ç½®
        self.seasonal_extractor = LastStepSeasonalExtractor(
            feature_dim=feature_dim,
            seq_len=seq_len,
            periods=seasonal_periods,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # 3. å±€éƒ¨æ®‹å·®æå–å™¨ - åªå…³æ³¨æœ€è¿‘çš„æ—¶é—´æ­¥
        self.residual_extractor = LastStepResidualExtractor(
            feature_dim=feature_dim,
            window_size=min(10, seq_len-1),  # æœ€å¤šçœ‹æœ€è¿‘10ä¸ªæ—¶é—´æ­¥
            num_heads=num_heads,
            dropout=dropout
        )
        
        # 4. ç®€åŒ–çš„ç»„ä»¶èåˆç½‘ç»œ
        self.fusion_layer = OptimizedComponentFusion(feature_dim)
        
    def forward(self, x, horizon="short"):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: åŸºç¡€æ¨¡å‹è¾“å‡ºçš„ç‰¹å¾ [batch_size, seq_len, feature_dim]
            horizon: é¢„æµ‹é•¿åº¦ç±»å‹ï¼Œ"short"æˆ–"long"
            
        Returns:
            final_features: åˆ†è§£åçš„æœ€åæ—¶é—´æ­¥ç‰¹å¾ [batch_size, feature_dim]
        """
        # æå–æœ€åæ—¶é—´æ­¥çš„ä¸‰ä¸ªç»„ä»¶
        trend_features = self.trend_extractor(x)
        seasonal_features = self.seasonal_extractor(x)
        residual_features = self.residual_extractor(x)
        
        # èåˆç»„ä»¶
        weights = torch.ones(3, device=x.device)
        if horizon == "long":
            # é•¿æœŸé¢„æµ‹æ›´å…³æ³¨è¶‹åŠ¿å’Œå­£èŠ‚æ€§
            weights = torch.tensor([0.5, 0.4, 0.1], device=x.device)
        else:
            # çŸ­æœŸé¢„æµ‹æ›´å¹³è¡¡
            weights = torch.tensor([0.3, 0.3, 0.4], device=x.device)
            
        final_features = self.fusion_layer(
            trend_features, seasonal_features, residual_features, weights, "adaptive"
        )
        
        return final_features


class LastStepTrendExtractor(nn.Module):
    """ä¼˜åŒ–çš„è¶‹åŠ¿æå–å™¨ï¼Œåªè®¡ç®—æœ€åæ—¶é—´æ­¥æ‰€éœ€çš„è¶‹åŠ¿ç‰¹å¾"""
    
    def __init__(self, feature_dim, seq_len, num_heads=4, dropout=0.1):
        super(LastStepTrendExtractor, self).__init__()
        self.feature_dim = feature_dim
        self.seq_len = seq_len
        
        
        # å¤šå¤´æ³¨æ„åŠ› - åªè®¡ç®—æœ€åä¸€ä¸ªæŸ¥è¯¢å‘é‡
        self.attn = EfficientCausalAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # åå¤„ç†ç½‘ç»œ
        self.norm = nn.LayerNorm(feature_dim)
        self.ffn = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        self.final_norm = nn.LayerNorm(feature_dim)
        
    def forward(self, x):
        """
        ä»…è®¡ç®—æœ€åæ—¶é—´æ­¥çš„è¶‹åŠ¿ç‰¹å¾
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, feature_dim]
            
        Returns:
            trend_features: æœ€åæ—¶é—´æ­¥çš„è¶‹åŠ¿ç‰¹å¾ [batch_size, feature_dim]
        """
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸æ·»åŠ ä½ç½®ç¼–ç ï¼‰
        x_pos = x
        
        # æå–æœ€åæ—¶é—´æ­¥çš„æŸ¥è¯¢å‘é‡
        last_query = x_pos[:, -1:, :]  # [batch_size, 1, feature_dim]
        
        # åº”ç”¨è¶‹åŠ¿åå¥½çš„æ³¨æ„åŠ› - åªè®¡ç®—æœ€åä¸€ä¸ªæŸ¥è¯¢
        trend_attn_output = self.attn(
            query=last_query,
            key_value=x_pos,
            is_trend=True
        )  # [batch_size, 1, feature_dim]
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        trend = self.norm(last_query + trend_attn_output)  # [batch_size, 1, feature_dim]
        
        # å‰é¦ˆç½‘ç»œ
        trend_ffn_output = self.ffn(trend)
        
        # æœ€ç»ˆè¶‹åŠ¿ç‰¹å¾
        trend_features = self.final_norm(trend + trend_ffn_output).squeeze(1)  # [batch_size, feature_dim]
        
        return trend_features


class LastStepSeasonalExtractor(nn.Module):
    """ä¼˜åŒ–çš„å­£èŠ‚æ€§æå–å™¨ï¼Œåªè®¡ç®—æœ€åæ—¶é—´æ­¥æ‰€éœ€çš„å­£èŠ‚æ€§ç‰¹å¾"""
    
    def __init__(self, feature_dim, seq_len, periods, num_heads=4, dropout=0.1):
        super(LastStepSeasonalExtractor, self).__init__()
        self.feature_dim = feature_dim
        self.seq_len = seq_len
        self.periods = periods
        
        # æ¯ä¸ªå‘¨æœŸçš„æ³¨æ„åŠ›æ¨¡å—
        self.attns = nn.ModuleList([
            EfficientCausalAttention(
                embed_dim=feature_dim,
                num_heads=num_heads,
                dropout=dropout
            )
            for _ in periods
        ])
        
        # åå¤„ç†ç½‘ç»œ
        self.norm = nn.LayerNorm(feature_dim)
        self.ffn = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        self.final_norm = nn.LayerNorm(feature_dim)
        
    def forward(self, x):
        """
        ä»…è®¡ç®—æœ€åæ—¶é—´æ­¥çš„å­£èŠ‚æ€§ç‰¹å¾
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, feature_dim]
            
        Returns:
            seasonal_features: æœ€åæ—¶é—´æ­¥çš„å­£èŠ‚æ€§ç‰¹å¾ [batch_size, feature_dim]
        """
        # æå–æœ€åæ—¶é—´æ­¥çš„æŸ¥è¯¢å‘é‡
        last_query = x[:, -1:, :]  # [batch_size, 1, feature_dim]
        
        # è®¡ç®—æ¯ä¸ªå‘¨æœŸçš„å­£èŠ‚æ€§ç»„ä»¶
        seasonal_outputs = []
        for i, period in enumerate(self.periods):
            # é€‰æ‹©å…³é”®çš„å‘¨æœŸæ€§ä½ç½®
            indices = []
            last_idx = self.seq_len - 1
            
            # ä»æœ€åæ—¶é—´æ­¥å‘å‰ï¼Œé€‰æ‹©æ‰€æœ‰åŒ¹é…å‘¨æœŸçš„ä½ç½®
            for j in range(min(5, self.seq_len // period)):  # æœ€å¤šçœ‹5ä¸ªå‘¨æœŸ
                idx = last_idx - period * (j + 1)
                if idx >= 0:
                    indices.append(idx)
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†å²ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
            if not indices:
                indices = list(range(self.seq_len - 1))
                
            # æ„å»ºé”®å€¼åºåˆ—ï¼ŒåªåŒ…å«ç›¸å…³å‘¨æœŸä½ç½®
            indices = [self.seq_len - 1] + indices  # åŒ…å«æœ€åæ—¶é—´æ­¥è‡ªèº«
            key_value = x[:, indices, :]  # [batch_size, len(indices), feature_dim]
            
            # åº”ç”¨å­£èŠ‚æ€§åå¥½çš„æ³¨æ„åŠ›
            seasonal_output = self.attns[i](
                query=last_query,
                key_value=key_value,
                is_trend=False,
                period=period
            )  # [batch_size, 1, feature_dim]
            
            seasonal_outputs.append(seasonal_output)
        
        # åˆå¹¶å¤šä¸ªå­£èŠ‚æ€§ç»„ä»¶
        if len(seasonal_outputs) > 1:
            seasonal = sum(seasonal_outputs) / len(seasonal_outputs)
        else:
            seasonal = seasonal_outputs[0]
            
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        seasonal = self.norm(last_query + seasonal)
        
        # å‰é¦ˆç½‘ç»œ
        seasonal_ffn_output = self.ffn(seasonal)
        
        # æœ€ç»ˆå­£èŠ‚æ€§ç‰¹å¾
        seasonal_features = self.final_norm(seasonal + seasonal_ffn_output).squeeze(1)
        
        return seasonal_features


class LastStepResidualExtractor(nn.Module):
    """ä¼˜åŒ–çš„æ®‹å·®æå–å™¨ï¼Œåªå…³æ³¨æœ€è¿‘å‡ ä¸ªæ—¶é—´æ­¥"""
    
    def __init__(self, feature_dim, window_size=10, num_heads=4, dropout=0.1):
        super(LastStepResidualExtractor, self).__init__()
        self.feature_dim = feature_dim
        self.window_size = window_size
        
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.attn = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # åå¤„ç†ç½‘ç»œ
        self.norm = nn.LayerNorm(feature_dim)
        self.ffn = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        self.final_norm = nn.LayerNorm(feature_dim)
        
    def forward(self, x):
        """
        ä»…è®¡ç®—æœ€åæ—¶é—´æ­¥çš„æ®‹å·®ç‰¹å¾
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, feature_dim]
            
        Returns:
            residual_features: æœ€åæ—¶é—´æ­¥çš„æ®‹å·®ç‰¹å¾ [batch_size, feature_dim]
        """
        batch_size, seq_len, _ = x.shape
        
        # åªä¿ç•™æœ€è¿‘çš„window_size+1ä¸ªæ—¶é—´æ­¥ï¼ˆåŒ…æ‹¬æœ€åä¸€æ­¥ï¼‰
        recent_window = max(0, seq_len - self.window_size - 1)
        x_recent = x[:, recent_window:, :]  # [batch_size, min(seq_len, window_size+1), feature_dim]
        
        # ä½¿ç”¨çª—å£æ•°æ®ï¼ˆä¸æ·»åŠ ä½ç½®ç¼–ç ï¼‰
        x_pos = x_recent
        
        # æå–æœ€åæ—¶é—´æ­¥çš„æŸ¥è¯¢å‘é‡
        last_query = x_pos[:, -1:, :]  # [batch_size, 1, feature_dim]
        
        # åˆ›å»ºæ®‹å·®æ©ç  - åªå…³æ³¨çŸ­æœŸä¾èµ–
        recent_len = x_recent.size(1)
        residual_mask = torch.ones(1, recent_len, device=x.device) * float('-inf')
        
        # æœ€è¿‘window_sizeä¸ªæ—¶é—´æ­¥å¯è§
        for i in range(min(self.window_size, recent_len-1)):
            # è·ç¦»è¶Šè¿‘æƒé‡è¶Šé«˜
            residual_mask[0, -(i+2)] = 1.0 - 0.8 * i / self.window_size
        
        # è‡ªèº«å¯è§
        residual_mask[0, -1] = 0.0
        
        # åº”ç”¨æ®‹å·®åå¥½çš„æ³¨æ„åŠ›
        residual_attn_output, _ = self.attn(
            query=last_query,
            key=x_pos,
            value=x_pos,
            attn_mask=residual_mask
        )  # [batch_size, 1, feature_dim]
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        residual = self.norm(last_query + residual_attn_output)
        
        # å‰é¦ˆç½‘ç»œ
        residual_ffn_output = self.ffn(residual)
        
        # æœ€ç»ˆæ®‹å·®ç‰¹å¾
        residual_features = self.final_norm(residual + residual_ffn_output).squeeze(1)
        
        return residual_features


class EfficientCausalAttention(nn.Module):
    """é«˜æ•ˆçš„å› æœæ³¨æ„åŠ›ï¼Œåªè®¡ç®—æœ€åä¸€ä¸ªæŸ¥è¯¢å‘é‡æ‰€éœ€çš„æ³¨æ„åŠ›"""
    
    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super(EfficientCausalAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # çº¿æ€§å˜æ¢
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key_value, is_trend=False, period=None):
        """
        é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œåªå…³æ³¨æœ€åä¸€ä¸ªæŸ¥è¯¢å‘é‡
        
        Args:
            query: æŸ¥è¯¢å‘é‡ï¼Œé€šå¸¸æ˜¯æœ€åæ—¶é—´æ­¥ [batch_size, 1, embed_dim]
            key_value: é”®å€¼åºåˆ— [batch_size, seq_len, embed_dim]
            is_trend: æ˜¯å¦ä¸ºè¶‹åŠ¿æ³¨æ„åŠ›
            period: å­£èŠ‚æ€§å‘¨æœŸ
            
        Returns:
            attn_output: æ³¨æ„åŠ›è¾“å‡º [batch_size, 1, embed_dim]
        """
        batch_size, seq_len, _ = key_value.shape
        assert query.size(1) == 1, "æŸ¥è¯¢åº”è¯¥åªåŒ…å«ä¸€ä¸ªæ—¶é—´æ­¥"
        
        # çº¿æ€§å˜æ¢
        q = self.q_proj(query)  # [batch_size, 1, embed_dim]
        k = self.k_proj(key_value)  # [batch_size, seq_len, embed_dim]
        v = self.v_proj(key_value)  # [batch_size, seq_len, embed_dim]
        
        # åˆ†ç¦»å¤´
        q = q.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, 1, head_dim]
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]
        
        # æ³¨æ„åŠ›åˆ†æ•°
        attn_weights = torch.matmul(q, k.transpose(-1, -2)) / (self.head_dim ** 0.5)  # [batch_size, num_heads, 1, seq_len]
        
        # åº”ç”¨ç‰¹å®šæ©ç 
        if is_trend:
            # è¶‹åŠ¿åå¥½ - è¿œè·ç¦»æƒé‡å¢å¼º
            weights = torch.ones(1, 1, 1, seq_len, device=query.device)
            for j in range(seq_len):
                # æœ€åä¸€ä¸ªæ—¶é—´æ­¥åˆ°å…¶ä»–æ­¥çš„è·ç¦»
                dist = seq_len - 1 - j
                # è·ç¦»è¶Šè¿œæƒé‡è¶Šé«˜
                weights[0, 0, 0, j] = 1.0 + 0.5 * dist / seq_len
            
            attn_weights = attn_weights * weights
            
        elif period is not None:
            # å­£èŠ‚æ€§åå¥½ - å‘¨æœŸä½ç½®å¢å¼º
            weights = torch.ones(1, 1, 1, seq_len, device=query.device) * 0.1  # åŸºç¡€æƒé‡ä½
            
            # å‘¨æœŸä½ç½®ç»™äºˆé«˜æƒé‡
            for j in range(seq_len):
                # è®¡ç®—ä¸æœ€åæ—¶é—´æ­¥çš„è·ç¦»
                dist = seq_len - 1 - j
                if dist > 0 and dist % period == 0:
                    weights[0, 0, 0, j] = 1.0  # å‘¨æœŸä½ç½®
            
            attn_weights = attn_weights * weights
        
        # åº”ç”¨softmax
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # æ³¨æ„åŠ›è¾“å‡º
        attn_output = torch.matmul(attn_weights, v)  # [batch_size, num_heads, 1, head_dim]
        
        # åˆå¹¶å¤´
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, 1, self.embed_dim)
        
        # è¾“å‡ºæŠ•å½±
        attn_output = self.out_proj(attn_output)
        
        return attn_output


class OptimizedComponentFusion(nn.Module):
    """ä¼˜åŒ–çš„ç»„ä»¶èåˆæ¨¡å—ï¼Œä¸“æ³¨äºæœ€åæ—¶é—´æ­¥"""
    
    def __init__(self, feature_dim):
        super(OptimizedComponentFusion, self).__init__()
        
        # ç»„ä»¶å˜æ¢
        self.trend_transform = nn.Linear(feature_dim, feature_dim)
        self.seasonal_transform = nn.Linear(feature_dim, feature_dim)
        self.residual_transform = nn.Linear(feature_dim, feature_dim)
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # åŠ¨æ€æƒé‡è°ƒæ•´ç½‘ç»œï¼ˆå¯é€‰ï¼Œä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
        self.weight_adjust = nn.Sequential(
            nn.Linear(feature_dim * 3, 3),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, trend, seasonal, residual, weights, horizon):
        """
        èåˆä¸‰ä¸ªç»„ä»¶
        
        Args:
            trend: è¶‹åŠ¿ç‰¹å¾ [batch_size, feature_dim]
            seasonal: å­£èŠ‚æ€§ç‰¹å¾ [batch_size, feature_dim]
            residual: æ®‹å·®ç‰¹å¾ [batch_size, feature_dim]
            weights: é»˜è®¤æƒé‡ [3]
            horizon: é¢„æµ‹ç±»å‹
            
        Returns:
            fused: èåˆåçš„ç‰¹å¾ [batch_size, feature_dim]
        """
        batch_size, feature_dim = trend.shape
        
        # åŠ¨æ€æƒé‡è°ƒæ•´ï¼ˆå¯é€‰ï¼‰
        if horizon == "adaptive":
            # è¿æ¥ç‰¹å¾è¯„ä¼°é‡è¦æ€§
            combined = torch.cat([trend, seasonal, residual], dim=-1)
            dynamic_weights = self.weight_adjust(combined)  # [batch_size, 3]
            
            # ç»“åˆé™æ€å’ŒåŠ¨æ€æƒé‡
            batch_weights = torch.zeros(batch_size, 3, device=trend.device)
            for b in range(batch_size):
                batch_weights[b] = (weights + dynamic_weights[b]) / 2
        else:
            # ä½¿ç”¨é™æ€æƒé‡
            batch_weights = weights.expand(batch_size, 3)
        
        # å˜æ¢å¹¶åŠ æƒ
        t = self.trend_transform(trend) * batch_weights[:, 0].unsqueeze(1)
        s = self.seasonal_transform(seasonal) * batch_weights[:, 1].unsqueeze(1)
        r = self.residual_transform(residual) * batch_weights[:, 2].unsqueeze(1)
        
        # èåˆ
        fused = self.fusion_layer(t + s + r)
        
        return fused


class PositionalEncoding(nn.Module):
    """é¢‘ç‡å¯è°ƒçš„ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=5000, freq_factor=1.0):
        super(PositionalEncoding, self).__init__()
        self.freq_factor = freq_factor
        
        # åˆ›å»ºä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model) * freq_factor
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        """æ·»åŠ ä½ç½®ç¼–ç åˆ°è¾“å…¥ç‰¹å¾"""
        return x + self.pe[:, :x.size(1), :]

3.ç›®å‰çš„å®éªŒç»“æœï¼š
é‡‡ç”¨DSsoftä½œä¸ºé›†æˆç­–ç•¥ï¼Œä»æŒ‡æ ‡ä¸Šå¯¹æ¯”åŸºæ¨¡å‹æ˜¯æœ‰æ•ˆæœçš„
Model	1~32		65~96	
    MSE	MAE	MSE	MAE
TimeMoE-50M	0.2983	0.3402	0.3985	0.4113
Near-head	0.2821	0.3497	0.5396	0.5478
Avg-head	0.3001	0.3464	0.4173	0.4265
Far-head	0.3491	0.4235	0.3682	0.4129
é›†æˆ	0.2753	0.3423	0.3634	0.4077
ä»ç‰¹å¾æ–¹é¢ï¼šç‰¹å¾åˆ†è§£åçš„Shortå’ŒLongç‰¹å¾ä¸åŸå§‹ç‰¹å¾çš„ç›¸ä¼¼åº¦åˆ†åˆ«ä¸º-0.198å’Œ-0.218ï¼Œæ¬§æ°è·ç¦»åˆ†åˆ«ä¸º30.04å’Œ30.27ï¼Œè¡¨æ˜åˆ†è§£åçš„ç‰¹å¾ä¸åŸå§‹ç‰¹å¾å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒæˆåŠŸå®ç°äº†ç‰¹å¾çš„ç‰¹åŒ–ã€‚Shortå’ŒLongç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦é«˜è¾¾0.966ï¼Œæ¬§æ°è·ç¦»ä¸º4.90ï¼Œè¡¨æ˜ä¸¤è€…ä¹‹é—´é«˜åº¦ç›¸ä¼¼ï¼Œè¿™å¯èƒ½é™åˆ¶äº†ä¸“å®¶å¤´çš„ç‰¹åŒ–æ•ˆæœï¼Œå¯¼è‡´ä¸“å®¶åœ¨ä¸åŒç‰¹å¾ä¸Šçš„åŒºåˆ†åº¦ä¸å¤Ÿæ˜æ˜¾ã€‚

